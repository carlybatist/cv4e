from opensoundscape.torch.models.cnn import CNN
from opensoundscape.audio import Audio
from opensoundscape.spectrogram import Spectrogram
from opensoundscape.annotations import BoxedAnnotations
from opensoundscape.annotations import categorical_to_one_hot
from opensoundscape.preprocess.utils import show_tensor_grid, show_tensor
from opensoundscape.torch.datasets import AudioFileDataset
import torch
import pandas as pd
from pathlib import Path
import numpy as np
import pandas as pd
import random
from glob import glob
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
plt.rcParams['figure.figsize']=[15,5]

# set manual seeds
#probably don’t want to do this when actually training model, but useful for debugging
torch.manual_seed(0)
random.seed(0)
np.random.seed(0)

#For this self-contained tutorial, we can use relative paths 
# (starting with a dot and referring to files in the same folder), 
# but you may want to use absolute paths for your training.

specky_table = pd.read_csv(Path("practice_one_hot_encoded_labels.csv"))
specky_table.head()
specky_table.file = ['./'+f for f in specky_table.file]
specky_table.head()
specky_table
specky_table_df = pd.DataFrame(specky_table)
specky_table_df
#audio = Audio.from_file("temp_clips/audiomoth12_20220226_140500.WAV")

# to create one-hot labels (already done from annotations script)
one_hot_labels, classes = categorical_to_one_hot(specky_table[['lemur']].values)
labels = pd.DataFrame(index=specky_table['file'],data=one_hot_labels,columns=classes)
labels.head()

# split training & test data
train_df,validation_df = train_test_split(labels,test_size=0.3,random_state=1)
train_df.head()
train_df
validation_df
print(f"created train_df (len {len(train_df)}) and validation_df (len {len(validation_df)})")

# Create model object
classes = train_df.columns

#syntax for creating model object
#arch = cnn_architectures.resnet50(num_classes=len(classes))
#model = cnn.CNN(arch,classes,sample_duration=2.0)
#model = cnn.CNN('resnet18',classes,2.0)
model = CNN('resnet18',classes=classes,sample_duration=5.0,single_target=True) #single-target (mutually exclusive classes)

# check what the samples generated by model look like

#pick some random samples from the training set
sample_of_4 = train_df.sample(n=4)
#generate a dataset with the samples we wish to generate and the model's preprocessor
inspection_dataset = AudioFileDataset(sample_of_4, model.preprocessor)
#generate the samples using the dataset
samples = [sample['X'] for sample in inspection_dataset]
labels = [sample['y'] for sample in inspection_dataset]
#display the samples
_ = show_tensor_grid(samples,4,labels=labels)

#turn augmentation off for the dataset
inspection_dataset.bypass_augmentations = True
#generate the samples without augmentation
samples = [sample['X'] for sample in inspection_dataset]
labels = [sample['y'] for sample in inspection_dataset]
#display the samples
_ = show_tensor_grid(samples,4,labels=labels)

# train model

model.train(
    train_df=train_df,
    validation_df=validation_df,
    save_path='./binary_train/', #where to save the trained model
    epochs=5,
    batch_size=8,
    save_interval=5, #save model every 5 epochs (the best model is always saved in addition)
    num_workers=0, #specify 4 if you have 4 CPU processes, eg; 0 means only the root process
)

#plot the loss from each epoch 
# (should decline as the model learns, but may have ups and downs along the way)
plt.scatter(model.loss_hist.keys(),model.loss_hist.values())
plt.xlabel('epoch')
plt.ylabel('loss')

#logging outputs
model.logging_level = 3 #request lots of logged content
model.log_file = './binary_train/training_log.txt' #specify a file to log output to
Path(model.log_file).parent.mkdir(parents=True,exist_ok=True) #make the folder ./binary_train

#Predict

#The predict function will internally split audio files into the appropriate length clips for 
#prediction and generate prediction scores for each clip.
field_recordings = glob('./Test/*')
field_recordings

prediction_scores_df, prediction_binary_df, unsafe_samples = model.predict(field_recordings)
prediction_scores_df.head()
prediction_scores_df
prediction_binary_df
unsafe_samples

#scores,preds,labels = model.predict(field_recordings, activation_layer='softmax', binary_preds='single_target')
scores,preds,labels = model.predict(field_recordings, binary_preds='single_target')
scores
preds

#setting multi-target makes model decide 0 or 1 based on threshold you can set
score_df, pred_df, label_df = model.predict(
    validation_df,
    binary_preds='multi_target',
    threshold=0.90,
)
pred_df
pred_df.head()


#modify the final activation layer to change the scores returned by the predict() function
#since we are choosing between two mutually exclusive classes, we want to use the 'softmax' activation.
#Since these samples are the same length as the training files, 
#we’ll specify split_files_into_clips=False (we just want one prediction per file, we don’t want to divide each file into shorter clips)
valid_scores, valid_preds, unsafe_samples = model.predict(
    validation_df,
    activation_layer='softmax',
    split_files_into_clips=False
)
valid_scores.columns = ['pred_negative','pred_positive']
validation_df.join(valid_scores).sample(10)

#OpenSoundscape saves models automatically during training:
#The model saves a copy of itself self.save_path to epoch-X.model automatically during training every save_interval epochs
#The model keeps the file best.model updated with the weights that achieve the best score on the validation dataset. 
# By default the model is evaluated using the mean average precision (MAP) score, 
# but you can overwrite model.eval() if you want to use a different metric for the best model.

#save the model manually at any time with model.save(path)
model1 = CNN('resnet18',classes,2.0,single_target=True)
# Save every 2 epochs
model1.train(
    train_df,
    validation_df,
    epochs=3,
    batch_size=8,
    save_path='./binary_train/',
    save_interval=2,
    num_workers=0
)
model1.save('./binary_train/my_favorite.model')

#Re-load a saved model with the load_model function:
from opensoundscape.torch.models.cnn import load_model
model = load_model('./binary_train/best.model')

#Using a saved or downloaded model to run predictions on audio files

#load the saved model
model = load_model('./binary_train/best.model')
#predict on a dataset
scores,_,_ = model.predict(field_recordings, activation_layer='softmax')

#Continue training from saved model

# .load() loads the entire model object, which includes optimizer parameters and learning rate parameters 
# from the saved model, in addition to the network weights.

#Create architecture
model = load_model('./binary_train/best.model')
#Continue training from the checkpoint where the model was saved
model.train(train_df,validation_df,save_path='.',epochs=0)


# Model training parameters
model.optimizer_params
#lr = learning rate
#how much the model’s weights change every time it calculates the loss function
#change
model.optimizer_params['lr']=0.001
#separate learning rates for feature & classifier blocks
#for ResNet architectures
from opensoundscape.torch.models.cnn import separate_resnet_feat_clf
r18_model = cnn.CNN('resnet18',classes,2.0)
print(r18_model.optimizer_params)
separate_resnet_feat_clf(r18_model) #in place operation!
r18_model.optimizer_params['feature']['lr'] = 0.001
r18_model.optimizer_params['classifier']['lr'] = 0.01
r18_model.optimizer_params

#momentum = learning rate schedule
#By default learning rates multiplied by 0.7 (the learning rate “cooling factor”) 
# once every 10 epochs (the learning rate “update interval”)
#often helpful to decrease the learning rate over the course of training
#change
model.lr_cooling_factor = 0.1
model.lr_update_interval = 1

#weight decay = regularization weight decay
#Pytorch optimizers perform L2 regularization, 
# giving the optimizer an incentive for the model to have small rather than large weights
# goal is to reduce overfitting
#change
model.optimizer_params['weight_decay']=0.001

#list embedded architectures
from opensoundscape.torch.architectures import cnn_architectures
cnn_architectures.list_architectures()
#resnet18, resnet34, resnet50, resnet101, resnet152
#alexnet, vgg11_bn, squeezenet1_0, densenet121, inception_v3

#could also create a custom architecture by subclassing an existing pytorch model 
# or writing one from scratch (the minimum requirement is that 
# it subclasses torch.nn.Module - it should at least have .forward() and .backward() methods.

#can choose whether to use pre-trained (ImageNet) weights 
my_arch = cnn_architectures.alexnet(num_classes=len(classes),use_pretrained=True)
# or start from scratch (use_pretrained=False for random weights)
my_arch = cnn_architectures.alexnet(num_classes=len(classes),use_pretrained=False)

#freezing the feature extractor
#if you only want to train the final classification layer of the network but not modify any other weights
#useful for applying pre-trained classifiers to new data, i.e. “transfer learning”
arch = cnn_architectures.resnet50(num_classes=10, freeze_feature_extractor=True, use_pretrained=False)



