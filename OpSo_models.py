from opensoundscape.torch.models.cnn import CNN
from opensoundscape.audio import Audio
from opensoundscape.spectrogram import Spectrogram
from opensoundscape.annotations import BoxedAnnotations
from opensoundscape.annotations import categorical_to_one_hot
from opensoundscape.preprocess.utils import show_tensor_grid, show_tensor
from opensoundscape.torch.datasets import AudioFileDataset
import torch
import pandas as pd
from pathlib import Path
import numpy as np
import pandas as pd
import random
from glob import glob
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
plt.rcParams['figure.figsize']=[15,5]

# set manual seeds
#probably donâ€™t want to do this when actually training model, but useful for debugging
torch.manual_seed(0)
random.seed(0)
np.random.seed(0)

#For this self-contained tutorial, we can use relative paths 
# (starting with a dot and referring to files in the same folder), 
# but you may want to use absolute paths for your training.

specky_table = pd.read_csv(Path("practice_one_hot_encoded_labels.csv"))
specky_table.head()
specky_table.file = ['./'+f for f in specky_table.file]
specky_table.head()
specky_table
specky_table_df = pd.DataFrame(specky_table)
#audio = Audio.from_file("temp_clips/audiomoth12_20220226_140500.WAV")

# to create one-hot labels (already done from annotations script)
one_hot_labels, classes = categorical_to_one_hot(specky_table[['lemur']].values)
labels = pd.DataFrame(index=specky_table['file'],data=one_hot_labels,columns=classes)
labels.head()

# split training & test data
train_df,validation_df = train_test_split(labels,test_size=0.2,random_state=1)
train_df.head()

# Create model object
classes = train_df.columns
model = CNN('resnet18',classes=classes,sample_duration=2.0,single_target=True)

# check what the samples generated by model look like

#pick some random samples from the training set
sample_of_4 = train_df.sample(n=4)
#generate a dataset with the samples we wish to generate and the model's preprocessor
inspection_dataset = AudioFileDataset(sample_of_4, model.preprocessor)
#generate the samples using the dataset
samples = [sample['X'] for sample in inspection_dataset]
labels = [sample['y'] for sample in inspection_dataset]
#display the samples
_ = show_tensor_grid(samples,4,labels=labels)

#turn augmentation off for the dataset
inspection_dataset.bypass_augmentations = True
#generate the samples without augmentation
samples = [sample['X'] for sample in inspection_dataset]
labels = [sample['y'] for sample in inspection_dataset]
#display the samples
_ = show_tensor_grid(samples,4,labels=labels)

# train model

model.train(
    train_df=train_df,
    validation_df=validation_df,
    save_path='./binary_train/', #where to save the trained model
    epochs=5,
    batch_size=8,
    save_interval=5, #save model every 5 epochs (the best model is always saved in addition)
    num_workers=0, #specify 4 if you have 4 CPU processes, eg; 0 means only the root process
)

#plot the loss from each epoch 
# (should decline as the model learns, but may have ups and downs along the way)
plt.scatter(model.loss_hist.keys(),model.loss_hist.values())
plt.xlabel('epoch')
plt.ylabel('loss')

model.logging_level = 3 #request lots of logged content
model.log_file = './binary_train/training_log.txt' #specify a file to log output to
Path(model.log_file).parent.mkdir(parents=True,exist_ok=True) #make the folder ./binary_train

#printing and logging outputs
model.verbose = 0 #don't print anything to the screen during training
model.train(
    train_df=train_df,
    validation_df=validation_df,
    save_path='./binary_train/', #where to save the trained model
    epochs=1,
    batch_size=8,
    save_interval=5, #save model every 5 epochs (the best model is always saved in addition)
    num_workers=0, #specify 4 if you have 4 CPU processes, eg; 0 means only the root process
)

